

spark 动态资源分配
磁盘文件可不可以跳读，跳读后是不是更快，如果是的话物化其实有作用
join 的物化可以减少 shuffle 传输数据量

hbase client 连同一个 hr 可以实现多线程么
多线程异步调用一个成功可以终止其它线程么，
可以利用 future 打断吗，如果在线程池中怎么处理

mysql的迭代器是一次查全量还是分批查
10000个futures遍历时，怎么知道哪个先返回呀
hive 窗口函数底层怎么实现的，太重要了


集群参数设置为所有map执行完再启reduce，防止map端倾斜（gzip）导致reduce 占资源
spark 粗粒度 只起一个server支持不了太多连接，但是启多个server实例又不合适，所以很难大面积应用
tez 只要启动一个连接就会启动一个 appmaster，公司对超过3个小时的就kill，所以也没有大面积应用
同比环比 留存

表join时 on处字段为空 where 字段为空对结果的影响 这个太重要了

mysql varchar长度设置大了会占用更多内存空间
丢包是真的丢数据吗

group by， join 时 null 引起数据倾斜

distributed by rand() 会不会有小文件
order by 加limit 会有优化
==================================================
百度外卖为什么用 kylin:
a. 流量分析的场景非常多，每种场景对数据的要求和分析角度都是不一样的。漏斗分析关注的是路径维度的pv/uv，订单、流水的趋势分析关注的是订单、
流水在时间上的变化，数据对比关注的是同样的指标和上周同期的对比。
b. 此外，流量分析的维度可以自由选择，这样使得查询场景更难固化。当然每种查询场景都可以通过定制化的SQL来聚合计算，但场景非常多，
如果只是在应用层去适配，开发成本太高。
c. kylin 的应该场景只是流量分析吧, 其它的还有什么呢
d. 路径维度其实就两个么, 9个页面, 每个页面 4 个属性, 结果搞出 9*4=36, 为什么要搞 36 个维度, 我觉得因为做流量分析, 要算各个路径的 pv uv 转化, 

曾用其它方案:
中间表方案: 我认为的中间表方案就是 select [所以维度], [所以指标] from tab group by [所有维度]

Kylin有5种立方体维度优化方案，维度聚合组、必选维度、层级维度、联合维度、派生维度。
维度聚合组：是对维度进行分组，以降低维度组合数目。如果有n+m个维度，如果将n个维度分为一组，m个维度分为另一组，则维度组合从2的n+m次方降为2的n次方加2的m次方。
必选维度：指所有的cuboid都包含的维度，每加一个必选维度，维度组合的数目就减少一半。
层级维度：指一系列具有层级关系的维度组成一个层级维度，设置了层级之后，对Cuboid增加了约束，低Level的维度一定要伴随高Level的维度出现，从而使维度组合大大减少。
联合维度：当有些维度始终在一起时，可以将这些维度设为联合维度。则任何有效的Cuboid要么不包含这些维度，要么包含所有维度，这些维度始终在一起，从而降低维度组合的数目。如果有n+m个维度，如果将m个维度配置成一个组合维度，则维度组合从2的n+m次方降为2的n+1次方。
派生维度：派生维度是针对维度表的，如果某张维度表有多个维度，如果该维度表一个或者多个列和维度表的主键是一一对应的，则可以将这些维度设置为派生维度。这样在Kylin内部会将其统一用维度表的主键来替换，以降低维度组合的数目。但查询效率会降低，因为Kylin在使用维度表主键进行聚合后，查询时需要再将主键转换为真正的维度列返回给用户

但采用了必选维度和层级维度这两种优化方案。INDEX_DAY（日期）和PATH_ID（路径ID）是必选维度，REGION_ID（大区ID）、CITY_ID（城市ID）、AOI_ID（商圈ID）是层级维度。同时维度表里的路径名称和路径编码采用的是派生维度
--我的思路：
  如果第一层对所有维度聚合肯定是不行的, 因为去重不了多少, 正常的用户不会在 app 上点来点去的
  但是如果第一层用除去 uv 的所有维度其实是可以的, 但是好像也不行, 因为漏斗转化也要带着 uv 吧
==========================================
Column-Store里面要把数据转换成Row的形式, 被称为物化(Materization)
经过我对列存储的了解和延迟物化的分析发现, 延迟物化没有多大的好处. 但是引发了我的一个思考, 多表非相同字段 join 时, 可以用单 reduce 来完成单 job
但是前提是其它表都与一个表 join 

==========================================
分区太多问题：1、动态分区全量时，写入时会相当慢 2、查全量时driver直接挂了
真对第一个问题可以做优化

2、mapjoin时将数据拉到driver（再通过driver发到其它机器，貌似hive命令行和hiveserver都有driver的概念，顺便说一下：
执行sql时报一个错，叫啥忘了，到指定（有权限）目录就不会有问题了，也是这个原因）

洗f_wide.wide_order时用sparksql比hive快5、6倍，用内存那种shuffle
为什么不是所有任务都用sparksql，因为spark是粗粒度的，只要启动就会占所有资源，如果数据倾斜，就不能搞了。而一些简单的join，用hive其实差不多，而且不会占用太多资源
ods 层为什么是外表，mysql增加列，删表的话数据没了


数据丢是因为实时特征这个需求，flink消费慢，给flink加了计算节点由18个变成32个，kafka分区只有18个，而且一个计算节点只能消费一个分区的数据，
所以加分区没有解决消费慢的问题，后面就给kafka新加了一个分区；这时候flink没有问题，可以自动发现新增的分区；
但是sparkstreaming那个收数的jar包得重启才能消费，到了第二天6点产品反馈数据少的时候，才想起这个问题，kafka的过期时间是24小时，这个时候超过24小时了，
所以丢了新增分区的数据

hdfs压缩文件问题是因为kafka数据堆积严重，想快点消费，把内存块称爆了，于是需要kill一些任务，本来那个kill脚本里面有个hook可以压缩完在kill，但是发现没有杀死，着急就用了kill -9 
--------
public static void main(String[] args) {
        String str = null;
        ff(str);
        System.out.println(str);//null
    }
    public static void ff(String str){
        str = "aaa";
    }
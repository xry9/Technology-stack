宽表意义重大，1、可以做到屏蔽底层业务变化的影响，2、口径肯定好之后，可以不去理会底层太零碎的业务(去哪儿)

处理数据时用 dw.dw_col_collector_info(没有 dt) 这种表的话，恢复历史数据会很麻烦。恢复历史数据情况：之前不统计 collector_id=320 (这是个 leader)，
后来又统计，恢复几个月的数据。所以数仓的上层数据也有必要做分区



如何减少上下文切换:
	避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据
	  引申：例如将本要存 LinkedBlockingQueue 的数据取 hash%[线程数] 后存入 HashMap 中, 其实最好有一个像 CurrentHashMap 那样分段的 Queue
====================================
开发平台:
	使用方: 其它 team 同学, 算法同学有些数据我们支持不了需要他们自己清洗, 业务线的数据分析师
	面临问题：管理不方便，给他们开权限，建表跑数据，不知道他们干了啥。开发平台就是要把口子收起来
				建表不规范, 不加注释，时间长了(没有生命周期)，自己都不知道用不用了。有些 sql 需要优化
	任务提交审核, 主要看 sql 是不是
	使用中的问题: 不知道任务怎样失败
	怎样 kill job, 怎样看日志
日志系统: 搜索相关日志，算法团队数据分析师经常用。搜索日志与业务日志关联算转化率
抽数系统: 分库分表

日常取数需求: 是否往返, 热门航线

线上应用: 火推机(), 支付干预(下单20min未支付, 可能在存在比价, 进行推券或打电话。画像系统记录历史航线点击)
		实时画像：主要针对营销场景, 新人高价值用户, 航线关注度 
		           风控场景: 黑名单用户

============================================
项目:
1、分表项目总结:
  a. 背景, 写用户行为数据, 磁盘快被打满, 数据只能存半年
  b. 方案:
    1) 数据库用双复本:
	  hbase 的 HFile 存在 hdfs 上, hbase 的读写全是基于 HFile(读了源码), 能不能双复本只需要看 hdfs 支不支持, hdfs 读写多复本是基于 pipline 策略(读源码)
      理论上多少个复本都可以, 与 NRW (zk)算法并不矛盾, 所以可行
	2) 如何删数据, hbase/scyllaDB delete 并不删数据, 要等 major compact 时才真正删除(还有 version), 采用时间抖动策略来触发的, compact 时会创建新的 HFile,  最好是
	  手动触发, 如果批量灌入数据, 造成大面积 region compact, 有一定风险
	  所以方案一就是手动触发 compact, 但是上百个 region 手动触发也很困难
	  方案二: 分表, 每月新建表删表, 但是又引出一个新问题, 不知道 key 属于哪张表
	    子方案一: 异步查询多张表, 哪个先返回非 null, 就取哪个, 因为不同表 region 分部也比较平均, 所以可以利用分部式的特点, 并发查询 1000 个 key 可以做到 2s 内响应
		子方案二: 每张表一个 bloom 过滤器, bloom 结果为 true 则去查询, 不为 true, 则过滤, 但是数据量太大, 可能内存压力太大, jvm Heap 越大, gc 压力
		就越大, 用堆外的话, 对机器压力也大, 但是如果并发量很大时, 还是得用这个方案
	3) 有个问我还做了哪些优化
		
2、要说下 hivemetastore 怎么改的源码: 拦截 socket 就行了
为什么要加版本号: 为了防止用户自行安装而跳过 sql 收集, sql 收集为实现全局血缘

3、开发平台: 
优点: 权限管理, sql 优化, 降低开发成本(shell 以前用户取数也不方便), 表结构查询建表规范, 构建数据血缘, 加上自助取数功能

hiveserver: 
负载均衡策略, Monitor 节点, 接收下面 hiveserver 的心跳, 上报内存(gc: ManagementFactory.getGarbageCollectorMXBeans())和 job/连接 数, 每秒 gc 不超过三次可用, 可以显示 gc 的区域和 gc 回收器名称
  hiveserver while 中加一个心跳方法, 有开关, 汇报连接数和 job 数, 用 GP , 定义一个 resourceBalance 协议 方法有 heartbeatReport, 返回值是个 command(shutdown)
  client 与 Monitor 之间就是一个 ResourceServer 协调, 方法是 getServer, Client 端还有一个延迟队列
FetchTask 的 maxRows 由 2000 --> 50
hdfs client 端的 buffer 改堆外
前端加 limit 限制
有了血缘调度怎么配法, 其实有两种方式, 配前置和不配前置
调度系统要有任务血缘, 表血缘字段血缘, 有些 spark 的血缘没法生成, 如果他们有可以问问


hivemetastore:
spark 等全都对接 hivemetastore
改源码两个, 一是 hivemetastore 截取 client IP, 二是客户端提交 sql 时, 用 sdk, 远程发送 qlibary, 所以要严格控制客户端版本

3、画像系统:

我们组没有产品，而产品还都是业务线的, 所以标签体系只能由我人技术来梳理, 很多标签都是基于需求的
建立一个完整的标签体系需要注重四点：了解标签的获得形式；清楚业务形态，以商业目的出发，汇集标签；对标签池进行分类和定义；标签的维护

标签体系(几类用户): 
C 端用户/乘机人: 用户近半年下单次数/下单金额/出行次数, 是否是学生用户/黄牛用户, 是否使用优惠券, 是否首次乘机
机场: 临近机场 机场周边(是否有大巴, 距市区距离)
航线: 航线热度(给用户展示)
航班: 延误率(给用户展示)

电商企业基本都实现了个性化实时营销，当用户准备购买一个商品，却在付款页面流失了，表明客户是有成单意愿，但存在某些疑虑，或者就是被打断后忘记回来了，
系统就会在大概十分钟之后，基本上等于实时给客户做营销推送，Push 用户成单    

标签的分类与定义
	人口统计标签:
	  通用属性：平台使用偏好: 否注册用户 最后一次访问时间 | 身份属性: 性别, 出生地, 学历, 婚姻, 是否学生
	交易标签: 购票次数, 航司偏好, 购票时间, 使用推送优惠券次数, 下单未支付次数, 可能会与实时标签相结合支持线上
	行为标签: app 启动次数, 时长, 登录次数, 浏览最多的, 连续 12 个月内飞行航段...  具体需要标签(最近一个月  下单未支付, 出国旅行次数)
	活动类标签: 去年 10.1/春节出行记录(城市/机场, 酒店可能来调用),
	基于挖掘类的标签: 该类标签为概率模型，概率是介于 0~1 之间的数值。例如：是白领/黄牛/商务人士的概率, 是男女的概率, 航司偏好, 返程的概率, 是否竞品(黑名单用户)
--------
标签系统:
标签库: 创建标签, 定义规则, 上线时间, 生命周期(因为具体需要标签/活动类标签), -- 要有审批
标签工厂: 跑数据, 配制调度
标签透镜镜(标签的维护) : 统计标签被调用次数, 基于接口日志
--------
4、实时标签:
架构: 行为数据实时采集好像真不是件容易的事, 
后端 SDK --> qlibary(留一份入 kudu 挂 impala, 行业中用 flume 的 log4j sdk avro) --> kafka
qlibary: pv vu
table1: qunar_username,arr_city,dep_city,dep_date,dom_inter,process(json),action(搜索 浏览 点击 下单),flight_type(单行/往返/一人/多人),lat,lng,ip
table2: 订单金额, 优惠券, 代金券
sdk --> qlibary server --> kafka --> spark 清洗(不同业务线数据整合) --> 1. kudu 2. kafka
  问问其它公司怎么做的, 实时标签等

5、实时系统分为两类: 
实时仓库(主要做统计):
	kudu 似乎是不二之选
	日志: 行为数据(后端日志, 流量分析) , 一天内的事件追加, rowkey 是 username, 列就是列, 值是时间加值
	  action 值往往存 json, 下单: order_id, click: 航班/航班集合, 解析航司就存 json, secrch: arr_city dep_city 中一个或两个, ota/浏览: 多个航班, 有航司就存 json
	binlog: 与离线数仓分层类似, 数据入 kudu, 做 olap 分析, 实时报表(标尺指标, 单量票量), 大屏, 也可以支持线上, 是否首次安装/下单
	  业务过程可能会很长, 所以一个订单存, 多条很有可能, 因为几乎没法做去重
	  实时表不好做整合, 表结构不可能完全一致, 交易表/订单明细表中业务过程该有的维度基本都有, 但主表和信息只有个 order_id 等, 没法 join 因为业务过程太长
	  或者不拉宽表, 表结构与业务线对应, loap 时自己 join 

实时标签(主要做用户维度的): 行为数据, 10/20/30min 窗口, 是面向需要并不是打标
  航司关注度(推送航司券, 或者是为航司或代理商引流, 用时需要比较历史下单信息), 下单概率(调用模型), 发券类型
  机场关注度: 是否推送临近机场, 是否推送给火车票(调用模型)
  航线偏好:
  生单未支付用户监控
  是否新用户/是否国航首次下单, 黑名单用户 这些不是用 spark 做, 但是属于实时标签

5、实时 olap 怎么做, 用 hbase 行不行, 对比一下 kudu
kudu 在 olap 方面比 hbase 要好
  a. 自己管理磁盘少了一层连接, 本地: SocketChannel vs FileChannel, buffer 都一样, DataNode 就多了一个 SocketChannel + FileChannel + buffer, 
	再加上 checksum 等因素相差应该是很多的
  b. 有 schema 列式存储, 而且 hbase 连列名都不知道, 解析文件还要复杂一些
  c. 在全表扫描方面也比 hbase 要好, 因为 hbase 有缓存
  d. Log Structured Merge Trees(LSM)

6、当被问数仓如何更好的落地或未来努力方向:
  a. 与业务形成闭环, 最好与业务实现绑定: ONP 是否黄牛 是否竞品 航司关注度(引流)
  b. 提升用户体验, 如 展示行程画像, 展示航班延误率, 提示机场周边
  c. 业务线之间互通, 机推火 火推机

12、自我介绍时要介绍一下我们部门干什么事, 数仓、平台、画像(实时/离线), 
国内 ONP: 下单后 30min 未支付, 发延迟消息, 可以用延迟队列, 也可以自己存 Hbase, 当然延迟队列底层可能就是存的 Hbase
14、抽数系统: 需要分布式, Reader Channel Writer, 涉及不到 nio 与零拷贝呀, 其实我觉得 hdfs Client 上传文件时可以用零拷贝
15、日志系统: 同样需要分布式, rsync 拉到本地, 再上传到 hdfs

16、
17、数据建模分层时怎么划分维度表事实表, 哪些杂项维度, 哪些微型维度, 哪些行为维度, 衍生维度要自己搞了, 但是建表时可以定义一个 schema
  没有衍生维度的话其它可以自动生成

19、多 count distinct 操作转 size collect_set 吧 
  select name, size(collect_set(age)),size(collect_set(dept)),size(collect_set(hobby)), count(1) from emp group by name;
  比 
  select name, count(distinct age),count(distinct dept),count(distinct hobby), count(1) from emp group by name;
  快了 N 倍
20、数据集市可以理解为是一个小型的部门或者工作组级别的数据仓库
  我理解数据集市主要是面向需求快速迭代
21、做问题的终结者别做为中转站啊, 要有 owner 意识。文件减值，只读一次
24、360 问 产品化 和 对数据建设有什么规划
  a. 数据面临 非全域问题, 造成计算浪费 口径不一 数据孤岛, 所以这些问题可通过 全域建模 解决
  b. 产品化还有个好处, 杂活/临时跑数 肯定会变少, 尤其是特征库啊, 因为产品化了数据分析平台(基于特征库的 olap 分析)
  c. 上面提到的 olap 要做好资源划分
  d. 数仓: 
    d1.数据质量、数据血缘, 对于数据质量可以定义表结构时定义一份字段范围的元数据
	d2. 是否有必要搞权限控制我还没想好, 应该是需要的只是我们还没有场景, hive sentry 的原理是什么, 只是在 hive 外包了一层吗
    d3. 表的输出要是严格的口径, 要提高复用性
    d4. 痛点: 
	  对于一个大 sql 的 etl, 改 sql 是个痛点
	  
27、hive with as, 但是 hive 好像默认有对这方面的优化
29、阿里说他们任务调度是走血缘关系的虚拟节点开始的, 上万个节点渲染不出来, 还说数据质量的校验能不能自动化, sum/count 等, 我觉得由于有分表, 
  这个真不太好做, 阿里内部的管理数仓就需要业务建模和领域建模了, 即得有 ER 建模, 而且这种系统可不可以搞出一个类似于中国移动的概念模型
30、hive join 时怎么做的一定要看一下
31、列转行用 select id, collect_list(concat(month,'-', salary)) from sal group by id;
32、实时需求 case by case 方式做
33、olap 场景有没有划分资源
34、被问到 rpc 为什么不用 http
35、马的, 留存不会算了, 告诉我不用开窗函数
36、快手小哥问表命名规范时说, 事实表也会有全量表吧, 我觉得他说得对
37、为什么不是所有任务都用 sparksql，因为spark是粗粒度的，只要启动就会占所有资源，如果数据倾斜，就不能搞了。而一些简单的join，用hive其实差不多，而且不会占用太多资源
38、东京面试小哥也是我当同事, 善意的提醒的简历格式实在是太烂了, 能让面试官提这种建议了说明我简历真的不是一般烂
39、遇到阿里头条等的面试官要问一下分桶有用的吗

41、数仓规范、特征库规范, 面试数仓架构师的岗位时被问到了
43、数据增删列也要有规范
  增加的列放在末尾，删除的列补默认值. 怎么维护历史元信息: 增加列没什么好说, 删除列其实也没什么大不了吧, 但是要做一个列变化的血统关系, 对变化特别频繁的要预警, 具体问题具体分析, 所以
  重建外表 修复分区
44、新浪小哥问 BIO 中调 interrupt 会怎么样
45、跟谁学大佬问, 一个复杂业务场景, 其实跟任勇在阿里面试类似, 写的一个最复杂的 sql, 这种问题其实并不突兀
46、京东大哥问, 快速查找某个商家下所有订单, 怎么设计 hbase rowkey. 我觉得应该是 商家id + order_id 做为 rowkey, 查的时候用 PrefixFilter. 这位京东大哥拿到我的简历我觉得很可能是上一次京东面试时前去哪儿同事面试官给我推的吧, 这如果是像这么靠谱的哥们儿不多呀
47、百度大哥问有没有遇到比较大的故障, 

================================================================

面试问题:
0、sql 优化, 一个大表要拆成多个小表, 怎样一次扫描就搞定
1、为什么流量分析把列搞那么多, 路径等
  --试想一下, 如果不这样, 我要 商圈(10), 页面(10), 版本(3), 渠道(3), 天(7), 路径(36) 的指标, 每次是不是得 group by 这些列, 
  产生 10*10*3*3*7*36 行数据
  考虑 uv 也没什么呀
2、hive 怎么实现 combiner, count distinct, 
3、spark 和 tez 生成 job 是不是一回事, 其实就是想知道 spark 在 shuffle 中, reduce 阶段是不是把数据落到 hdfs 上了
5、hbase 事务, mysql 分表事务, hbase 的 wal 表明 hdfs 数据不可以追加, 但是可以 put 都更新到 DN, 但是 packet 是怎么做到的呢
6、大数据量 bloomFilter join 还是很有用的, 算是不能 map join 的一种折衷(绿湾有这种场景, 一张非常大的用户维表)
7、关于 reduce 数是否可以动态调整的问题, 我觉得可以让 AppMaster 来做, map 执行完要向 AppMaster 汇报数据量
	set spark.sql.adaptive.enabled; // 是否开启调整partition功能，如果开启，spark.sql.shuffle.partitions设置的partition可能会被合并到一个reducer里运行
	set spark.sql.adaptive.shuffle.targetPostShuffleInputSize; //开启spark.sql.adaptive.enabled后，两个partition的和低于该阈值会合并到一个reducer
数据倾斜:
SparkSQL 自适应框架可以根据预先的配置在作业运行过程中自动检测是否出现倾斜，并对检测到的倾斜进行优化处理
优化的主要逻辑是对倾斜的 partition 进行拆分由多个 task 来进行处理，最后通过union 进行结果合并
https://zhuanlan.zhihu.com/p/79665055
8、生单和退款这两个业务 为什么不放在同个表里: 这涉及其实是一个生命周期管理，和模型易用度的问题

8、
orc 文件根据不同的列, id/deptName/addr, distribute by addr sort by addr, 后文件大小. 少了 sort by 不行,只 sort by 效果同相,但是多 reduce 时应该还是不同
 7.9 M 2020-05-13 19:27 /hive1/user/hive/warehouse/db.db/deptxx_orc/000000_0
 6.9 M 2020-05-13 19:31 /hive1/user/hive/warehouse/db.db/deptxx_orc/000000_0
11.7 M 2020-05-13 19:33 /hive1/user/hive/warehouse/db.db/deptxx_orc/000000_0
但是我觉得列多了也没多大作用了
我觉得 distribute by 最多的场景就是在 insert 操作时作打散操作吧, 在业务上没有意义, 与 mapred.reduce.tasks 配合, 防止数据都进一个 reduce, 
防止产生大文件, 在非 shuffle 操作中提交并行度

9、磁盘是否支持并行访问
  我写代码真没测出来, 差异很大, 不可信
10、hdfs 有几个 DN 挂了还继续可用, 我的第一感觉是 hdfs 三副本只要有一个好的就能用, 分析一下 hdfs 在 cap 中对 a/可用性 保障的较差, cp 是保障的
11、面试中有说一站式数据服务平台, 其实就是数据中台么
12、连接池/线程池关闭, 多线程异步调用关闭


我觉得一个很大的问题了, 数仓遇到现有业务表很难完成数据分析工作时, 数据组没有给业务上表设计提意见的

1、bin/start-cluster.sh
2、web端口8081
----分部式----
vim flink-conf.yaml
jobmanager.rpc.address: cloud01
vim slaves
bin/start-cluster.sh

flink run -c com.xryj.atGuiGu.wc.StreamWordCount app/mygit/FlinkSimple/target/FlinkSimple-1.0-SNAPSHOT.jar -p 2	--xx 
	-p 大于 1 的话需要调大 taskmanager.numberOfTaskSlots
flink list
flink cancel f7e146f55594c9fe97e9a8ee3c6e9bd3
flink list --all
https://www.jianshu.com/p/1b05202c4fb6

拷贝 jar, flink-shaded-hadoop-2-uber-2.7.2-10.0.jar(apache/flink-shaded/tree/release-10.0, 自行修改 hadoop 版本)
--------------- yarn----------------
不用做配制, 因为有 HADOOP_HOME, 如果启动过 yarn-session.sh, 需要 rm -f /tmp/.yarn-properties-tyx, 有此文件会优先提交到 yarn 上
初步认定 flink 代码中并没有加载 hadoop 配置, 只是引用 hadoop jar, 可以说完全是 hadoop 自身机制加载

./bin/yarn-session.sh -n 3 -tm 4096 -s 
flink run -c com.xryj.flink.SocketTextStreamWordCount flink-demo/target/flink-demo-1.8.2.jar
FlinkYarnSessionCli
YarnSessionClusterEntrypoint
YarnTaskExecutorRunner

flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 -c com.xryj.flink.SocketTextStreamWordCount flink-demo/target/flink-demo-1.8.2.jar
--YarnSessionClusterEntrypoint

mvn clean install -DskipTests && cd /usr/local/app/mygit/flink182-simple/flink-dist/ && mvn clean package -DskipTests && cp target/flink-dist_2.11-1.8.2.jar /usr/local/app/flink-1.8.2/lib/flink-dist_2.11-1.8.2.jar && scp target/flink-dist_2.11-1.8.2.jar pseudo2:/usr/local/app/flink-1.8.2/lib/flink-dist_2.11-1.8.2.jar && scp target/flink-dist_2.11-1.8.2.jar pseudo3:/usr/local/app/flink-1.8.2/lib/flink-dist_2.11-1.8.2.jar && cd -

checkpoint blob updateGlobalAggregate


===============================
flink-1.8.2:
注释掉 flink-dist 中的 dependency, 会少一些 flink 的那些 jar
1、vim bin/config.sh  .*flink-dist.*.jar --> .*flink-.*.jar
2、vim bin/flink-daemon.sh "$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS"`:/usr/local/app/flink-1.8.2/flink-dist_2.11-1.8.2.jar"。 同样 flink 脚本也要加上

flink-1.13.2:
三个 jar 特别处理: flink-dist_2.11-1.13.2.jar flink-table_2.11-1.13.2.jar flink-table-blink_2.11-1.13.2.jar。删除 org/apache/flink, dist 下的 shade 不要删
cp */target/flink*jar /usr/local/app/flink-1.13.2/lib/lib1
cp */*/target/flink*jar /usr/local/app/flink-1.13.2/lib/lib2/
删除 flink-fs-hadoop-shaded-1.13.2.jar, example, test 等, 加入 flink-runtime-web_2.11-1.13.2.jar 因为本地无法编译
包含除 org/apache/flink 包外其它包的工程: flink-fs-hadoop-shaded flink-s3-fs-base flink-s3-fs-presto flink-runtime flink-table-planner flink-table-blink
flink-sql 连接 kafka 
flink-sql-connector-kafka_2.11-1.13.2.jar
kafka-clients-2.8.0.jar
mysql-connector-java-5.1.37-bin.jar
flink-connector-jdbc_2.11-1.13.2.jar
flink-connector-kafka_2.11-1.13.2.jar
calcite-core-1.26.0.jar
guava-29.0-jre.jar


=====================================
https://www.cnblogs.com/moonlight-lin/p/12873594.html
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/try-flink/flink-operations-playground/


State Backends: heap-based, RocksDB
Checkpoint Storage:  one that persists its state snapshots to a distributed file system, and another that users the JobManager’s heap
State Snapshots: 
Snapshot – a generic term referring to a global, consistent image of the state of a Flink job. A snapshot includes a pointer into each of the data sources (e.g., an offset into a file or Kafka partition), as well as a copy of the state from each of the job’s stateful operators that resulted from having processed all of the events up to those positions in the sources.
Checkpoint – a snapshot taken automatically by Flink for the purpose of being able to recover from faults. Checkpoints can be incremental, and are optimized for being restored quickly.
Externalized Checkpoint – normally checkpoints are not intended to be manipulated by users. Flink retains only the n-most-recent checkpoints (n being configurable) while a job is running, and deletes them when a job is cancelled. But you can configure them to be retained instead, in which case you can manually resume from them.
Savepoint – a snapshot triggered manually by a user (or an API call) for some operational purpose, such as a stateful redeploy/upgrade/rescaling operation. Savepoints are always complete, and are optimized for operational flexibility.

Queryable state allows you to access state from outside of Flink during runtime
***https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/concepts/stateful-stream-processing/
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/concepts/flink-architecture/
The jobs of a Flink Application can either be submitted to a long-running Flink Session Cluster, a dedicated Flink Job Cluster, or a Flink Application Cluster. The difference between these options is mainly related to the cluster’s lifecycle and to resource isolation guarantees.

The execution mode can be configured via the execution.runtime-mode setting. There are three possible values: STREAMING, Batch, AUTOMATIC
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/fault-tolerance/broadcast_state/
externalized checkpoints: You can configure periodic checkpoints to be persisted externally. Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the job fails. This way, you will have a checkpoint around to resume from if your job fails
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/process_function/
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/asyncio/
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/dataset/overview/	--dataset api 部分没有看
https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/concepts/dynamic_tables/

----
bin/start-scala-shell.sh remote pseudo 8081
val textStreaming = senv.fromElements("To be, or not to be,--that is the question:--","Whether 'tis nobler in the mind to suffer","The slings and arrows of outrageous fortune","Or to take arms against a sea of troubles,")
val countsStreaming = textStreaming.flatMap { _.toLowerCase.split("\\W+") }.map { (_, 1) }.keyBy(0).sum(1)
countsStreaming.print()
senv.execute("Streaming Wordcount")

bin/flink run ./examples/batch/WordCount.jar --input file:///home/tyx/data.txt --output file:///home/tyx/wordcount_out1






================================================================

https://github.com/flink-china/flink-training-course
https://ci.apache.org/projects/flink/flink-docs-release-1.7/concepts/programming-model.html
The number of operator subtasks is the parallelism of that particular operator. The parallelism of a stream is always that of its producing operator. Different operators of the same program may have different levels of parallelism.
Windows can be time driven (example: every 30 seconds) or data driven (example: every 100 elements). One typically distinguishes different types of windows, such as tumbling windows (no overlap), sliding windows (with overlap), and session windows (punctuated by a gap of inactivity).
More window examples can be found in this blog post. 
Stateful Operations
Flink implements fault tolerance using a combination of stream replay and checkpointing
	https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html
Batch on Streaming
	 A DataSet is treated internally as a stream of data. The concepts above thus apply to batch programs in the same way as well as they apply to streaming programs


https://ci.apache.org/projects/flink/flink-docs-release-1.7/concepts/runtime.html
Tasks and Operator Chains
Task Slots and Resources
	 Slotting the resources means that a subtask will not compete with subtasks from other jobs for managed memory, but instead has a certain amount of reserved managed memory. Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks.
	 Tasks in the same JVM share TCP connections (via multiplexing) and heartbeat messages. They may also share data sets and data structures, thus reducing the per-task overhead.
Savepoints
**** Note that no CPU isolation happens here; currently slots only separate the managed memory of tasks.

https://ci.apache.org/projects/flink/flink-docs-release-1.7/tutorials/datastream_api.html
mvn exec:java -Dexec.mainClass=wikiedits.WikipediaAnalysis

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/examples.html#running-an-example
Page Rank
Connected Components

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/projectsetup/scala_api_quickstart.html
	mvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-scala -DarchetypeVersion=1.7.1
	Run the quickstart script
Inspect Project
Build Project
mvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-scala -DarchetypeVersion=1.7.1
For Eclipse, you need the following plugins, which you can install from the provided Eclipse Update Sites:

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/projectsetup/dependencies.html
Note on IntelliJ: To make the applications run within IntelliJ IDEA, the Flink dependencies need to be declared in scope compile rather than provided. Otherwise IntelliJ will not add them to the classpath and the in-IDE execution will fail with a NoClassDefFountError. To avoid having to declare the dependency scope as compile (which is not recommended, see above), the above linked Java- and Scala project templates use a trick: They add a profile that selectively activates when the application is run in IntelliJ and only then promotes the dependencies to scope compile, without affecting the packaging of the JAR files.
Hadoop Dependencies
Appendix: Template for building a Jar with Dependencies

It is highly recommended to keep the dependencies in scope provided. If they are not set to provided, the best case is that the resulting JAR becomes excessively large, because it also contains all Flink core dependencies. The worst case is that the Flink core dependencies that are added to the application��s jar file clash with some of your own dependency versions (which is normally avoided through inverted classloading).
Note on IntelliJ: To make the applications run within IntelliJ IDEA, the Flink dependencies need to be declared in scope compile rather than provided. Otherwise IntelliJ will not add them to the classpath and the in-IDE execution will fail with a NoClassDefFountError. To avoid having to declare the dependency scope as compile (which is not recommended, see above), the above linked Java- and Scala project templates use a trick: They add a profile that selectively activates when the application is run in IntelliJ and only then promotes the dependencies to scope compile, without affecting the packaging of the JAR files.
Adding Connector and Library Dependencies
If you need Hadoop dependencies during testing or development inside the IDE (for example for HDFS access), please configure these dependencies similar to the scope of the dependencies to test or to provided.
To build an application JAR that contains all dependencies required for declared connectors and libraries, you can use the following shade plugin definition:

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/api_concepts.html
Define keys for Tuples
General Class Types
Values
Type Erasure & Type Inference
Accumulators & Counters

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/datastream_api.html
you can always write your own custom sources by implementing the SourceFunction for non-parallel sources, or by implementing the ParallelSourceFunction interface or extending the RichParallelSourceFunction for parallel sources.
IMPLEMENTATION:
IMPORTANT NOTES:
Execution Parameters
Note that the write*() methods on DataStream are mainly intended for debugging purposes. They are not participating in Flink��s checkpointing, this means these functions usually have at-least-once semantics. The data flushing to the target system depends on the implementation of the OutputFormat. This means that not all elements send to the OutputFormat are immediately showing up in the target system. Also, in failure cases, those records might be lost.


https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/projectsetup/java_api_quickstart.html
mvn archetype:generate  -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-java -DarchetypeVersion=1.7.1
We recommend you import this project into your IDE to develop and test it. IntelliJ IDEA supports Maven projects out of the box. If you use Eclipse, the m2e plugin allows to import Maven projects. Some Eclipse bundles include that plugin by default, others require you to install it manually.
A note to Mac OS X users: The default JVM heapsize for Java may be too small for Flink. You have to manually increase it. In Eclipse, choose Run Configurations -> Arguments and write into the VM Arguments box: -Xmx800m. In IntelliJ IDEA recommended way to change JVM options is from the Help | Edit Custom VM Options menu. See this article for details.

https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/event_timestamps_watermarks.html
Stream sources can directly assign timestamps to the elements they produce, and they can also emit watermarks. When this is done, no timestamp assigner is needed. Note that if a timestamp assigner is used, any timestamps and watermarks provided by the source will be overwritten.
https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/state/state.html

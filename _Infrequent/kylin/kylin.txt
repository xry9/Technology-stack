在mapred-site.xml中加入
<property>
<name>mapreduce.jobhistory.address</name>
<value>pseudo:10020</value>
<description>MapReduce JobHistory Server IPC host:port</description>
</property>
<!--property>
<name>mapred.child.tmp</name>
<value>/usr/local/app/hadoop-2.7.2/temp</value> 
</property-->
在yarn-site.xml中加入
<property>
<name>yarn.log-aggregation-enable</name>    
<value>true</value>    
</property>
<property>
<name>yarn.log.server.url</name>
<value>http://pseudo:19888/jobhistory/logs</value>
</property>

mr-jobhistory-daemon.sh start historyserver
kylin.sh start
sample.sh

----
多表join貌似对kylin很致命

kylin_account
account_id	account_buyer_level	account_seller_level	account_country	account_contact
10000000	1	4	FR	N/A
10000001	0	1	DE	N/A

kylin_sales
trans_id	part_dt	lstg_format_name	leaf_categ_id	lstg_site_id	slr_segment_cd	price	item_count	seller_id	buyer_id ops_user_id	ops_region
0	2012-12-14	Others	88750	0	11	36.2828	4	10000349	10002313	ANALYST	Beijing
1	2012-08-28	Others	175750	0	13	23.8563	20	10000927	10004376	ANALYST	Beijing

20180101000000_20180606000000

=======================================================
1、OLAP面临问题：
	1、随着维度的不断增加，在数仓中维护各种维度组合的聚合表的成本越来越高，数据开发效率明显下降
	2、数据量超过千万行后，MySQL的导入和查询变得非常慢，经常把MySQL搞崩
2、美团OLAP特点：
	20个左右维度，50个左右指标，包含大量去重指标，结果要求精确
	面向内部PM/运营，响应时间要求3s内

为了支持这些需求，我们的线上环境包含一个30节点的Kylin专属HBase集群，2台用于Cube构建的物理机，和8台8核16G的VM用作Kylin的查询机。Cube的构建是运行在主计算集群的MR作业，各业务线的构建任务拆分到了他们各自的资源队列上

提到MOLAP Cube方案，很多没接触过Kylin的人会担心“维度爆炸”的问题，即每增加一个维度，由于维度组合数翻倍，Cube的计算和存储量也会成倍增长。我们起初其实也有同样的担心，但调研和使用Kylin一阵子后发现，这个问题在实践中并没有想象的严重。这主要是因为
	1、Kylin支持Partial Cube，不需要对所有维度组合都进行预计算
	2、实际业务中，维度之间往往存在衍生关系，而Kylin可以把衍生维度的计算从预计算推迟到查询处理阶段


===============================================


流量分析的数据为什么有这么多维度呢？其实主要是路径维度太多，有36个路径维度。经过分析，可以将这36个路径维度可以聚合成一个路径ID，一个路径ID表示一种路径，路径的具体信息存放在路径维度表中，事实表通过路径ID和维度表关联。我们经常关注的路径只有几百个，所以这个维度的基数不会太大
必选维度，层级维度，联合维度，派生维度
该表包括的维度字段有：path_id、区域id、城市id、商圈id、来源、app版本、app渠道、小时、订单状态、日期。指标字段包括：用户的cuid、优惠前价格、百度补贴、商户补贴、代理商补贴、订单id


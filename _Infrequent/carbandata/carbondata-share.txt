http://carbondata.apache.org/file-structure-of-carbondata.html

1、支持Update and Delete
	http://carbondata.apache.org/introduction.html
	CarbonData supports Update and Delete on Big Data.CarbonData provides the syntax similar to Hive to support IUD operations on CarbonData tables.

	http://carbondata.apache.org/dml-of-carbondata.html
	
	UPDATE t3 SET (t3_salary) = (t3_salary + 9) WHERE t3_name = 'aaa1'
	UPDATE t3 SET (t3_date, t3_country) = ('2017-11-18', 'india') WHERE t3_salary < 15003
	UPDATE t3 SET (t3_country, t3_name) = (SELECT t5_country, t5_name FROM t5 WHERE t5_id = 5) WHERE t3_id < 5
	UPDATE t3 SET (t3_date, t3_serialname, t3_salary) = (SELECT '2099-09-09', t5_serialname, '9999' FROM t5 WHERE t5_id = 5) WHERE t3_id < 5
	UPDATE t3 SET (t3_country, t3_salary) = (SELECT t5_country, t5_salary FROM t5 FULL JOIN t3 u WHERE u.t3_id = t5_id and t5_id=6) WHERE t3_id >6

	DELETE FROM carbontable WHERE column1  = 'china'
	DELETE FROM carbontable WHERE column1 IN ('china', 'USA')
	DELETE FROM carbontable WHERE column1 IN (SELECT column11 FROM sourceTable2)
	DELETE FROM carbontable WHERE column1 IN (SELECT column11 FROM sourceTable2 WHERE column1 = 'USA')

2、当streaming segment达到最大值，状态会变为"streaming finish"，创建新的streaming segment继续接收流式数据
http://carbondata.apache.org/streaming-guide.html
After the streaming segment reaches the max size, CarbonData will change the segment status to "streaming finish" from "streaming", and create new "streaming" segment to continue to ingest streaming data.

	import java.io.File
	import org.apache.spark.sql.{CarbonEnv, SparkSession}
	import org.apache.spark.sql.CarbonSession._
	import org.apache.spark.sql.streaming.{ProcessingTime, StreamingQuery}
	import org.apache.carbondata.core.util.path.CarbonTablePath
	import org.apache.carbondata.streaming.parser.CarbonStreamParser

	val spark = SparkSession.builder().getOrCreateCarbonSession
	spark.sql(s"DROP TABLE IF EXISTS carbon_table")
	spark.sql(s"""CREATE TABLE carbon_table (col1 STRING,col2 STRING)STORED AS carbondata TBLPROPERTIES('streaming'='true','carbon.streaming.segment.max.size'='1024000')""".stripMargin)
	val carbonTable = CarbonEnv.getCarbonTable(Some("default"), "carbon_table")(spark)
	val tablePath = carbonTable.getTablePath

	var qry: StreamingQuery = null
	val readSocketDF = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("kafka.startingoffsets", "earliest").option("subscribe", "kt01").load().selectExpr("CAST(value AS STRING)","CAST(value AS STRING)")
	qry = readSocketDF.writeStream.format("carbondata").trigger(ProcessingTime("5 seconds")).option("checkpointLocation", CarbonTablePath.getStreamingCheckpointDir(tablePath)).option("dbName", "default").option("tableName", "carbon_table").option(CarbonStreamParser.CARBON_STREAM_PARSER, CarbonStreamParser.CARBON_STREAM_PARSER_CSV).start()

	new Thread() {
	override def run(): Unit = {
	do {
	spark.sql("select * from carbon_table").show(false)
	Thread.sleep(10000)
	} while (true)
	}
	}.start()
	spark.sql("select count(1) from carbon_table").show
	spark.sql("select col1 from carbon_table where col1='223000'").show


3、text类型支持lucene索引
Lucene is popular for indexing text data which are long.CarbonData provides a lucene datamap so that text columns can be indexed using lucene and use the index result for efficient pruning of data to be retrieved during query.

4、与spark,presto深度集成，但与hive集成不好
Refer to Integration with Spark, Presto for detailed information on integrating CarbonData with these execution engines.

5、布隆过滤器
http://carbondata.apache.org/bloomfilter-datamap-guide.html
A Bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. Carbondata introduced BloomFilter as an index datamap to enhance the performance of querying with precise value. It is well suitable for queries that do precise match on high cardinality columns(such as Name/ID). Internally, CarbonData maintains a BloomFilter per blocklet for each index column to indicate that whether a value of the column is in this blocklet. Just like the other datamaps, BloomFilter datamap is managed along with main tables by CarbonData. User can create BloomFilter datamap on specified columns with specified BloomFilter configurations such as size and probability.
CREATE TABLE datamap_test (
  id string,
  name string,
  age int,
  city string,
  country string)
STORED AS carbondata
TBLPROPERTIES('SORT_COLUMNS'='id')
In the above example, id and name are high cardinality columns and we always query on id and name with precise value. since id is in the sort_columns and it is orderd, query on it will be fast because CarbonData can skip all the irrelative blocklets. But queries on name may be bad since the blocklet minmax may not help, because in each blocklet the range of the value of name may be the same -- all from A* to z*. In this case, user can create a BloomFilter datamap on column name. Moreover, user can also create a BloomFilter datamap on the sort_columns. This is useful if user has too many segments and the range of the value of sort_columns are almost the same.

User can create BloomFilter datamap using the Create DataMap DDL:

CREATE DATAMAP dm
ON TABLE datamap_test
USING 'bloomfilter'
DMPROPERTIES ('INDEX_COLUMNS' = 'name,id', 'BLOOM_SIZE'='640000', 'BLOOM_FPP'='0.00001', 'BLOOM_COMPRESS'='true')

6、预聚合
http://carbondata.apache.org/preaggregate-datamap-guide.html
Once pre-aggregate datamaps are created, CarbonData's SparkSQL optimizer extension supports to select the most efficient pre-aggregate datamap and rewrite the SQL to query against the selected datamap instead of the main table. Since the data size of pre-aggregate datamap is smaller, user queries are much faster. In our previous experience, we have seen 5X to 100X times faster in production SQLs.


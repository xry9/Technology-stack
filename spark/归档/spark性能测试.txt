val wc=sc.textFile("/data/words0.txt",4).flatMap{x=>x.split(" ")}.map{x=>(x,1)}.reduceByKey((x,y)=>x+y,8)
wc:1082892663,1.0 G
18/03/25 20:03:42 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 20:06:11 INFO mapreduce.Job:  map 79% reduce 21%
18/03/25 20:07:09 INFO mapreduce.Job:  map 100% reduce 29%
18/03/25 20:08:04 INFO mapreduce.Job:  map 100% reduce 100%
多map，cpu几乎满
总用时：4:22
map用时：3:27
交插用时：58+
3.3min 33s
3.5min 43s

gz:376.4 M
18/03/25 18:27:48 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 18:34:06 INFO mapreduce.Job:  map 100% reduce 0%
18/03/25 18:34:38 INFO mapreduce.Job:  map 100% reduce 100%
单map，cpu为25%-30%
总用时：6:50
map用时：6:18
交插用时：0
3.6min 19s
3.7min 32s

bzp2:300.2 M
18/03/25 18:37:37 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 18:39:34 INFO mapreduce.Job:  map 56% reduce 11%
18/03/25 18:43:08 INFO mapreduce.Job:  map 100% reduce 11%
18/03/25 18:43:51 INFO mapreduce.Job:  map 100% reduce 100%
3map，前期cpu80%,reduce启动后将满
总用时：6:14
map用时：5:31
交插用时：3:34
5.2min 26s
5.2min 41s

lz4:629.6 M
18/03/25 18:46:43 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 18:52:58 INFO mapreduce.Job:  map 100% reduce 0%
18/03/25 18:53:33 INFO mapreduce.Job:  map 100% reduce 100%
单map，cpu为25%-30%
总用时：6:50
map用时：6:15
交插用时：0
3.3min 18s
3.6min 30s

sn:627.2 M
18/03/25 18:55:22 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 19:01:32 INFO mapreduce.Job:  map 100% reduce 0%
18/03/25 19:02:10 INFO mapreduce.Job:  map 100% reduce 100%
单map，cpu近30%
总用时：6:48
map用时：6:10
交插用时：0
3.6min 14s
3.6min 30s

lzo:617498968,588.9 M
18/03/25 23:18:30 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 23:24:47 INFO mapreduce.Job:  map 100% reduce 0%
18/03/25 23:25:20 INFO mapreduce.Job:  map 100% reduce 100%
结果也被压缩了
单map，cpu为25%-30%
总用时：6:50
map用时：6:17
交插用时：0
3.8min 19s
3.7min 29s

lzop:617499010,588.9 M
18/03/25 23:27:06 INFO mapreduce.Job:  map 0% reduce 0%
18/03/25 23:33:04 INFO mapreduce.Job:  map 100% reduce 0%
18/03/25 23:33:40 INFO mapreduce.Job:  map 100% reduce 100%
结果也被压缩了 大小同lzo结果
单map，cpu为25%-30%
总用时：6:34
map用时：5:58
交插用时：0
3.4min 18s
3.2min 29s
===========================================================
Array(((User4772,Location16),CompactBuffer((2018-01-05 00:21:10,12), (2017-01-14 18:44:06,30), (2017-05-25 13:40:28,24), (2017-12-24 06:19:48,1), (2017-10-10 12:48:37,29), (2017-04-28 12:34:18,21), (2018-03-15 09:34:53,21), (2017-06-04 06:31:42,49), (2017-12-19 00:23:21,24), (2017-12-06 17:51:50,27), (2017-04-13 08:46:51,2), (2018-01-18 10:52:43,51), (2017-09-11 18:46:14,34), (2017-10-15 20:36:42,3), (2018-02-12 21:59:55,14), (2017-07-25 22:50:08,50), (2017-04-26 19:30:10,17), (2017-03-30 09:33:30,12))), ((User9923,Location19),CompactBuffer((2018-02-05 01:42:33,23), (2017-02-20 03:15:52,38), (2017-01-17 20:25:09,36), (2017-02-11 03:20:12,24))))


val rdd =sc.makeRDD(List(("cat",2),("dog",3),("tiger",6),("cat",3),("tiger",4),("dog",5),("tiger",2),("cat",3)),2)
rdd.groupByKey.mapValues { x => x.toList.sorted.take(1) }


User7425,Location60,2018-01-25 18:11:11,32
User7425,Location60,2017-04-04 23:21:15,51
User7425,Location60,2017-08-21 22:34:27,52
User7425,Location60,2017-12-10 01:25:46,22
User7425,Location60,2018-03-05 20:46:44,25
User7425,Location60,2017-03-17 17:20:24,58

sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");((x(0),x(1)),(x(2),x(3).toInt))}).reduceByKey((x,y)=>(if(x._1.compareTo(y._1)>0) y._1 else x._1,x._2+y._2)).take(1)
sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).toDF("userid","location","time","period").groupBy("userid","location").agg(sum("period")).limit(1).show

sc.textFile("hdfs://myhdfs/input.csv").map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).toDF("userid","location","time","period").groupBy("userid","location").agg(min("time").alias("timeMin"),sum("period").alias("pseiodSum")).write.saveAsTable("userLocation")
.map(x=>x.split(",")).map(x=>((x(0),x(1)),(x(2),x(3).toInt))).reduceByKey((x,y)=>(if(x._1.compareTo(y._1)>0) y._1 else x._1,x._2+y._2)).saveAsTextFile("/result/userLocation")


spark.sqlContext.load("/data2").toDF("userid","location","time","period").groupBy("userid","location").agg(min("time"),sum("period")).limit(1).show

case class Coltest(col1:String,col2:String,col3:String,col4:Int)extends Serializable
sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).map(line=>Coltest(line._1,line._2,line._3,line._4)).toDS

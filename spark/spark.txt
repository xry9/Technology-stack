动态分区，小文件处理，集群资源划分，
1、单个超市在任意连续7天内，下相同订单数（相同商品相同商品价格）≥10个视为异常
orderid,sid,gid,time
select sid,gid,time,count(orderid) from sid,gid,time

6.多超市下单时间相同
同一服务站下，有3个及以上的超市，下单(在线完成订单)天数≥7（一个月内下单次数超过7天），（超市之间7次出现这种现象）
订单生成时间间隔和完成时间间隔均相差15分钟以内。
lateral view explode

===========================================================
Array(((User4772,Location16),CompactBuffer((2018-01-05 00:21:10,12), (2017-01-14 18:44:06,30), (2017-05-25 13:40:28,24), (2017-12-24 06:19:48,1), (2017-10-10 12:48:37,29), (2017-04-28 12:34:18,21), (2018-03-15 09:34:53,21), (2017-06-04 06:31:42,49), (2017-12-19 00:23:21,24), (2017-12-06 17:51:50,27), (2017-04-13 08:46:51,2), (2018-01-18 10:52:43,51), (2017-09-11 18:46:14,34), (2017-10-15 20:36:42,3), (2018-02-12 21:59:55,14), (2017-07-25 22:50:08,50), (2017-04-26 19:30:10,17), (2017-03-30 09:33:30,12))), ((User9923,Location19),CompactBuffer((2018-02-05 01:42:33,23), (2017-02-20 03:15:52,38), (2017-01-17 20:25:09,36), (2017-02-11 03:20:12,24))))

val rdd =sc.makeRDD(List(("cat",2),("dog",3),("tiger",6),("cat",3),("tiger",4),("dog",5),("tiger",2),("cat",3)),2)
rdd.groupByKey.mapValues { x => x.toList.sorted.take(1) }

User7425,Location60,2018-01-25 18:11:11,32
User7425,Location60,2017-04-04 23:21:15,51
User7425,Location60,2017-08-21 22:34:27,52
User7425,Location60,2017-12-10 01:25:46,22
User7425,Location60,2018-03-05 20:46:44,25
User7425,Location60,2017-03-17 17:20:24,58

sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");((x(0),x(1)),(x(2),x(3).toInt))}).reduceByKey((x,y)=>(if(x._1.compareTo(y._1)>0) y._1 else x._1,x._2+y._2)).take(1)
sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).toDF("userid","location","time","period").groupBy("userid","location").agg(sum("period")).limit(1).show

sc.textFile("hdfs://myhdfs/input.csv").map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).toDF("userid","location","time","period").groupBy("userid","location").agg(min("time").alias("timeMin"),sum("period").alias("pseiodSum")).write.saveAsTable("userLocation")
.map(x=>x.split(",")).map(x=>((x(0),x(1)),(x(2),x(3).toInt))).reduceByKey((x,y)=>(if(x._1.compareTo(y._1)>0) y._1 else x._1,x._2+y._2)).saveAsTextFile("/result/userLocation")


spark.sqlContext.load("/data2").toDF("userid","location","time","period").groupBy("userid","location").agg(min("time"),sum("period")).limit(1).show

case class Coltest(col1:String,col2:String,col3:String,col4:Int)extends Serializable
sc.textFile("/data/spark.txt",128).map(line=>{val x=line.split(",");(x(0),x(1),x(2),x(3).toInt)}).map(line=>Coltest(line._1,line._2,line._3,line._4)).toDS


-----------------
spark 用linux命令：
scala> import sys.process._
import sys.process._
scala> "hdfs dfs -ls /ML/front_page/ATR".!

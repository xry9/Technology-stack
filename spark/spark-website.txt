http://spark.apache.org/docs/latest/sql-getting-started.html
Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object
Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection-based approach leads to more concise code and works well when you already know the schema while writing your Spark application.
The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.
Untyped User-Defined Aggregate Functions
Type-Safe User-Defined Aggregate Functions
http://spark.apache.org/docs/latest/sql-data-sources-parquet.html
http://spark.apache.org/docs/latest/sql-performance-tuning.html
http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html
http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html

http://spark.apache.org/docs/latest/rdd-programming-guide.html
In the Spark shell, a special interpreter-aware SparkContext is already created for you, in the variable called sc. Making your own SparkContext will not work. You can set which master the context connects to using the --master argument, and you can add JARs to the classpath by passing a comma-separated list to the --jars argument. You can also add dependencies (e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates to the --packages argument. Any additional repositories where dependencies might exist (e.g. Sonatype) can be passed to the --repositories argument. For example, to run bin/spark-shell on exactly four cores, use:
Typically you want 2-4 partitions for each CPU in your cluster
Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.
By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.
display in the web UI for the stage that modifies that accumulator
For other Hadoop InputFormats, you can use the SparkContext.hadoopRDD method, which takes an arbitrary JobConf and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use SparkContext.newAPIHadoopRDD for InputFormats based on the “new” MapReduce API (org.apache.hadoop.mapreduce).
The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to organize the data, and a set of reduce tasks to aggregate it.
Certain shuffle operations can consume significant amounts of heap memory since they employ in-memory data structures to organize records before or after transferring them. Specifically, reduceByKey and aggregateByKey create these structures on the map side, and 'ByKey operations generate these on the reduce side. When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.
**Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the spark.local.dir configuration parameter when configuring the Spark context.
**Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.
**Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.

http://spark.apache.org/docs/latest/configuration.html
The application web UI at http://<driver>:4040 lists Spark properties in the “Environment” tab. This is a useful place to check to make sure that your properties have been set correctly. Note that only values explicitly specified through spark-defaults.conf, SparkConf, or the command line will appear. For all other configuration properties, you can assume the default value is used.

spark.driver.memory	--> In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file. 
spark.executor.memory
spark.executor.memoryOverhead
spark.local.dir
**spark.driver.supervise
spark.driver.extraClassPath
spark.executor.extraJavaOptions
spark.jars.packages

**spark.reducer.maxSizeInFlight
spark.maxRemoteBlockSizeFetchToMem
**spark.shuffle.file.buffer
spark.shuffle.io.numConnectionsPerPeer
spark.shuffle.io.preferDirectBufs
spark.shuffle.service.enabled
spark.shuffle.sort.bypassMergeThreshold
spark.io.compression.zstd.level
spark.kryo.classesToRegister
spark.kryo.registrationRequired

spark.memory.fractionExecution Behavior
spark.memory.storageFraction
spark.memory.offHeap.enabled
spark.memory.offHeap.size
spark.memory.useLegacyMode
spark.shuffle.memoryFraction
spark.storage.memoryFraction
spark.storage.unrollFraction
spark.storage.replication.proactive
spark.cleaner.periodicGC.interval
spark.cleaner.referenceTracking 	
spark.cleaner.referenceTracking.blocking
spark.cleaner.referenceTracking.blocking.shuffle
spark.cleaner.referenceTracking.cleanCheckpoints
spark.executor.cores
spark.default.parallelism
spark.files.useFetchCache
spark.files.maxPartitionBytes
spark.files.openCostInBytes
spark.hadoop.validateOutputSpecs
spark.storage.memoryMapThreshold
spark.network.timeout
spark.locality.wait
spark.scheduler.minRegisteredResourcesRatio
spark.scheduler.mode
spark.blacklist.enabled
spark.task.cpus
spark.speculation.quantile
spark.task.reaper.enabled
spark.dynamicAllocation.enabled

Running the SET -v command will show the entire list of the SQL configuration.
spark.streaming.backpressure.enabled 	
spark.streaming.blockInterval
spark.streaming.receiver.maxRate
spark.streaming.receiver.writeAheadLog.enable
spark.streaming.unpersist
spark.streaming.kafka.maxRatePerPartition
spark.streaming.kafka.maxRetries

spark.deploy.recoveryMode
spark.deploy.zookeeper.url

Inheriting Hadoop Cluster Configuration
Custom Hadoop/Hive Configuration


Spark properties mainly can be divided into two kinds: one is related to deploy, like “spark.driver.memory”, “spark.executor.instances”, this kind of properties may not be affected when setting programmatically through SparkConf in runtime, or the behavior is depending on which cluster manager and deploy mode you choose, so it would be suggested to set through configuration file or spark-submit command line options; another is mainly related to Spark runtime control, like “spark.task.maxFailures”, this kind of properties can be set in either way.
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file. 
spark.extraListeners
spark.log.callerContext
spark.driver.supervise
spark.driver.extraClassPath
spark.executor.extraJavaOptions
spark.files
spark.jars
spark.jars.packages
spark.jars.excludes
spark.jars.repositories

spark.reducer.maxSizeInFlight
spark.maxRemoteBlockSizeFetchToMem
spark.shuffle.service.enabled
spark.shuffle.registration.timeout
spark.shuffle.registration.maxAttempts


http://spark.apache.org/docs/latest/tuning.html
Kryo serialization: Spark can also use the Kryo library (version 4) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all Serializable types and requires you to register the classes you’ll use in the program in advance for best performance.
** You can switch to using Kryo by initializing your job with a SparkConf and calling conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer"). This setting configures the serializer used for not only shuffling data between worker nodes but also when serializing RDDs to disk. The only reason Kryo is not the default is because of the custom registration requirement, but we recommend trying it in any network-intensive application. Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.
**Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.
Memory Tuning
Memory Management Overview
Tuning Data Structures
Garbage Collection Tuning
Memory Usage of Reduce Tasks
Spark prints the serialized size of each task on the master, so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing.




http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
1、The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. 
	we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. 
2、This model is significantly different from many other stream processing engines. Many streaming systems require the user to maintain running aggregations themselves, thus having to reason about fault-tolerance, and data consistency (at-least-once, or at-most-once, or exactly-once). In this model, Spark is responsible for updating the Result Table when there is new data, thus relieving the users from reasoning about it. As an example, let’s see how this model handles event-time based processing and late arriving data.
5、Fault Tolerance Semantics
5、Schema inference and partition of streaming DataFrames/Datasets
7、Conditions for watermarking to clean aggregation state
8、
Inner Joins with optional Watermarking
While the watermark + event-time constraints is optional for inner joins, for left and right outer joins they must be specified. This is because for generating the NULL results in outer join, the engine must know when an input row is not going to match with anything in future. Hence, the watermark + event-time constraints must be specified for generating correct results
Additional details on supported joins:
Some sinks (e.g. files) may not supported fine-grained updates that Update Mode requires. To work with them, we have also support Append Mode, where only the final counts are written to sink. This is illustrated below.
7、Arbitrary Stateful Operations
7、Policy for handling multiple watermarks
Unsupported Operations


5、http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
6、These examples generate streaming DataFrames that are untyped, meaning that the schema of the DataFrame is not checked at compile time, only checked at runtime when the query is submitted. Some operations like map, flatMap, etc. need the type to be known at compile time. To do those, you can convert these untyped streaming DataFrames to typed streaming Datasets using the same methods as static DataFrame
7、Streaming Deduplication
8、Trigger interval: Optionally, specify the trigger interval. If it is not specified, the system will check for availability of new data as soon as the previous processing has completed. If a trigger time is missed because the previous processing has not completed, then the system will trigger processing immediately.

8、Different types of streaming queries support different output modes. Here is the compatibility matrix.
	Queries with aggregation 	的 Aggregation on event-time with watermark 的Append与Update有什么区别
	mapGroupsWithState
		https://github.com/apache/spark/blob/v2.4.0/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala
Using Foreach and ForeachBatch
8、Structured Streaming supports joining a streaming Dataset/DataFrame with a static Dataset/DataFrame as well as another streaming Dataset/DataFrame. 

    By default, foreachBatch provides only at-least-once write guarantees. However, you can use the batchId provided to the function as way to deduplicate the output and get an exactly-once guarantee.
    foreachBatch does not work with the continuous processing mode as it fundamentally relies on the micro-batch execution of a streaming query. If you write data in the continuous mode, use foreach instead.
	If foreachBatch is not an option (for example, corresponding batch data writer does not exist, or continuous processing mode), then you can express you custom writer logic using foreach. Specifically, you can express the data writing logic by dividing it into three methods: open, process, and close. Since Spark 2.4, foreach is available in Scala, Java and Python.
Execution semantics When the streaming query is started, Spark calls the function or the object’s methods in the following way:
Triggers
** Reading Metrics Interactively
You can also asynchronously monitor all queries associated with a SparkSession by attaching a StreamingQueryListener
Reporting Metrics using Dropwizard
Recovering from Failures with Checkpointing
Recovery Semantics after Changes in a Streaming Query
Caveats





http://spark.apache.org/docs/latest/streaming-programming-guide.html
	http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html
1、http://spark.apache.org/docs/latest/streaming-custom-receivers.html
Receiver Reliability

2、
	dstream.foreachRDD { rdd =>
	  val connection = createNewConnection()  // executed at the driver（原来这driver端）
	  rdd.foreach { record =>
		connection.send(record) // executed at the worker
	  }
	}
Finally, this can be further optimized by reusing connection objects across multiple RDDs/batches. One can maintain a static pool of connection objects than can be reused as RDDs of multiple batches are pushed to the external system, thus further reducing the overheads.
For window-based operations like reduceByWindow and reduceByKeyAndWindow and state-based operations like updateStateByKey, this is implicitly true. Hence, DStreams generated by window-based operations are automatically persisted in memory, without the developer calling persist().

3、Note that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.
5、For input streams that receive data over the network (such as, Kafka, Flume, sockets, etc.), the default persistence level is set to replicate the data to two nodes for fault-tolerance.
Checkpointing
https://github.com/apache/spark/blob/v2.4.0/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala
Configuring automatic restart of the application driver - To automatically recover from a driver failure, the deployment infrastructure that is used to run the streaming application must monitor the driver process and relaunch the driver if it fails. Different cluster managers have different tools to achieve this. 
Configuring write-ahead logs
**Setting the max receiving rate
Upgrading Application Code
	The checkpoint information essentially contains serialized Scala/Java/Python objects and trying to deserialize objects with new, modified classes may lead to errors. In this case, either start the upgraded app with a different checkpoint directory, or delete the previous checkpoint directory.
Monitoring Applications
Setting the right batch size such that the batches of data can be processed as fast as they are received (that is, data processing keeps up with the data ingestion).
Level of Parallelism in Data Receiving
Level of Parallelism in Data Processing
Data Serialization
Task Launching Overheads
There are a few parameters that can help you tune the memory usage and GC overheads:
Important points to remember:
	When data is received from a stream source, receiver creates blocks of data. A new block of data is generated every blockInterval milliseconds. N blocks of data are created during the batchInterval where N = batchInterval/blockInterval. These blocks are distributed by the BlockManager of the current executor to the block managers of other executors. After that, the Network Input Tracker running on the driver is informed about the block locations for further processing.
	An RDD is created on the driver for the blocks created during the batchInterval. The blocks generated during the batchInterval are partitions of the RDD. Each partition is a task in spark. blockInterval== batchinterval would mean that a single partition is created and probably it is processed locally.
Fault-tolerance Semantics
Basic Semantics
With Receiver-based Sources
With Kafka Direct API
Semantics of output operations

6、Data checkpointing - Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains
7、For stateful transformations that require RDD checkpointing, the default interval is a multiple of the batch interval that is at least 10 seconds
8、Accumulators, Broadcast Variables, and Checkpoints
9、Configuring automatic restart of the application driver - To automatically recover from a driver failure, the deployment infrastructure that is used to run the streaming application must monitor the driver process and relaunch the driver if it fails. Different cluster managers have different tools to achieve this.
10、Upgrading Application Code
11、The following two metrics in web UI are particularly important:
12、The number of blocks in each batch determines the number of tasks that will be used to process the received data in a map-like transformation
13、In specific cases where the amount of data that needs to be retained for the streaming application is not large, it may be feasible to persist data (both types) as deserialized objects without incurring excessive GC overheads. For example, if you are using batch intervals of a few seconds and no window operations, then you can try disabling serialization in persisted data by explicitly setting the storage level accordingly. This would reduce the CPU overheads due to serialization, potentially improving performance without too much GC overheads.

http://spark.apache.org/docs/latest/submitting-applications.html
you can also specify --supervise to make sure that the driver is automatically restarted if it fails with a non-zero exit code
If you are ever unclear where configuration options are coming from, you can print out fine-grained debugging information by running spark-submit with the --verbose option.
When using spark-submit, the application jar along with any jars included with the --jars option will be automatically transferred to the cluster. URLs supplied after --jars must be separated by commas. That list is included in the driver and executor classpaths. Directory expansion does not work with --jars.
Spark uses the following URL scheme to allow different strategies for disseminating jars:
Note that JARs and files are copied to the working directory for each SparkContext on the executor nodes. This can use up a significant amount of space over time and will need to be cleaned up. With YARN, cleanup is handled automatically, and with Spark standalone, automatic cleanup can be configured with the spark.worker.cleanup.appDataTtl property.
Users may also include any other dependencies by supplying a comma-delimited list of Maven coordinates with --packages. All transitive dependencies will be handled when using this command. Additional repositories (or resolvers in SBT) can be added in a comma-delimited fashion with the flag --repositories. (Note that credentials for password-protected repositories can be supplied in some cases in the repository URI, such as in https://user:password@host/.... Be careful when supplying credentials this way.) These commands can be used with pyspark, spark-shell, and spark-submit to include Spark Packages.
application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.
Advanced Dependency Management

http://spark.apache.org/docs/latest/spark-standalone.html
Additionally, standalone cluster mode supports restarting your application automatically if it exited with non-zero exit code. To use this feature, you may pass in the --supervise flag to spark-submit when launching your application. Then, if you wish to kill an application that is failing repeatedly, you may do so through:
The number of cores assigned to each executor is configurable. When spark.executor.cores is explicitly set, multiple executors from the same application may be launched on the same worker if the worker has enough cores and memory. Otherwise, each executor grabs all the cores available on the worker by default, in which case only one executor per application may be launched on each worker during one single schedule iteration.
Standby Masters with ZooKeeper
Single-Node Recovery with Local File System


http://spark.apache.org/docs/latest/running-on-yarn.html
The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration.
In cluster mode, the driver runs on a different machine than the client, so SparkContext.addJar won’t work out of the box with files that are local to the client. To make files on the client available to SparkContext.addJar, include them with the --jars option in the launch command.
To make Spark runtime jars accessible from YARN side, you can specify spark.yarn.archive or spark.yarn.jars. For details please refer to Spark Properties. If neither spark.yarn.archive nor spark.yarn.jars is specified, Spark will create a zip file with all jars under $SPARK_HOME/jars and upload it to the distributed cache.
In YARN terminology, executors and application masters run inside “containers”. YARN has two modes for handling container logs after an application has completed. If log aggregation is turned on (with the yarn.log-aggregation-enable config), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the yarn logs command.
Debugging your Application
Important notes
Configuring the External Shuffle Service
Using the Spark History Server to replace the Spark Web UI

http://spark.apache.org/docs/latest/hardware-provisioning.html
	In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the rest for the operating system and buffer cache.

http://spark.apache.org/docs/latest/job-scheduling.html
The simplest option, available on all cluster managers, is static partitioning of resources. With this approach, each application is given a maximum amount of resources it can use and holds onto them for its whole duration.
YARN: The --num-executors option to the Spark YARN client controls how many executors it will allocate on the cluster (spark.executor.instances as configuration property), while --executor-memory (spark.executor.memory configuration property) and --executor-cores (spark.executor.cores configuration property) control the resources per executor. 
A second option available on Mesos is dynamic sharing of CPU cores. In this mode, each Spark application still has a fixed and independent memory allocation (set by spark.executor.memory), but when the application is not running tasks on a machine, other applications may run tasks on those cores. This mode is useful when you expect large numbers of not overly active applications, such as shell sessions from separate users. However, it comes with a risk of less predictable latency, because it may take a while for an application to gain back cores on one node when it has work to do. To use this mode, simply use a mesos:// URL and set spark.mesos.coarse to false.
Note that none of the modes currently provide memory sharing across applications. If you would like to share data this way, we recommend running a single server application that can serve multiple requests by querying the same RDDs.
Dynamic Resource Allocation
Configuration and Setup
Resource Allocation Policy
Request Policy
Remove Policy
Graceful Decommission of Executors
Scheduling Within an Application
Fair Scheduler Pools
Default Behavior of Pools
Configuring Pool Properties
Scheduling using JDBC Connections

http://spark.apache.org/docs/latest/monitoring.html
You can access this interface by simply opening http://<driver-node>:4040 in a web browser. If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc).
Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.
Viewing After the Fact
Environment Variables
Spark History Server Configuration Options
REST API
Executor Task Metrics
Metrics
Advanced Instrumentation

private[deploy] class org.apache.spark.deploy.worker.Worker(override val rpcEnv: RpcEnv, webUiPort: Int, cores: Int, memory: Int, masterRpcAddresses: Array[RpcAddress], endpointName: String, workDirPath: String = null, val conf: SparkConf, val securityMgr: SecurityManager) extends ThreadSafeRpcEndpoint with Logging {
	override def receive: PartialFunction[Any, Unit] = synchronized {
		case LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_) =>
			val manager = new ExecutorRunner(appId, execId, appDesc.copy(command = Worker.maybeUpdateSSLSettings(appDesc.command, conf)), cores_, memory_, self, workerId, host, webUi.boundPort, publicAddress, sparkHome, executorDir, workerUri, conf, appLocalDirs, ExecutorState.RUNNING)
			executors(appId + "/" + execId) = manager
			manager.start()
			sendToMaster(ExecutorStateChanged(appId, execId, manager.state, None, None))
	}
}
private[deploy] class ExecutorRunner(val appId: String, val execId: Int, val appDesc: ApplicationDescription, val cores: Int, val memory: Int, val worker: RpcEndpointRef, val workerId: String, val host: String, val webUiPort: Int, val publicAddress: String, val sparkHome: File, val executorDir: File, val workerUrl: String, conf: SparkConf, val appLocalDirs: Seq[String], @volatile var state: ExecutorState.Value) extends Logging {
	private[worker] def start() {
		workerThread = new Thread("ExecutorRunner for " + fullId) {
			override def run() { fetchAndRunExecutor() }
		}
		workerThread.start()
	}
	private def fetchAndRunExecutor() {
		val builder = CommandUtils.buildProcessBuilder(appDesc.command, new SecurityManager(conf), memory, sparkHome.getAbsolutePath, substituteVariables)
		val command = builder.command()// "/usr/local/app/jdk1.8.0_77/bin/java" "-cp" "/usr/local/app/spark-2.2.0-bin-hadoop2.7/conf/:/usr/local/app/spark-2.2.0-bin-hadoop2.7/jars/*:/usr/local/app/hadoop-2.7.2/etc/hadoop/" "-Xmx1024M" "-Dspark.driver.port=53463" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.58.140:53463" "--executor-id" "0" "--hostname" "192.168.58.140" "--cores" "4" "--app-id" "app-20200512021608-0000" "--worker-url" "spark://Worker@192.168.58.140:49097"
		process = builder.start()
		val exitCode = process.waitFor()
		worker.send(ExecutorStateChanged(appId, execId, state, Some(message), Some(exitCode)))
	}
}


private[deploy] class org.apache.spark.deploy.worker.Worker(override val rpcEnv: RpcEnv, webUiPort: Int, cores: Int, memory: Int, masterRpcAddresses: Array[RpcAddress], endpointName: String, workDirPath: String = null, val conf: SparkConf, val securityMgr: SecurityManager, resourceFileOpt: Option[String] = None, externalShuffleServiceSupplier: Supplier[ExternalShuffleService] = null) extends ThreadSafeRpcEndpoint with Logging {
	override def receive: PartialFunction[Any, Unit] = synchronized {
		case LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_, resources_) =>
			val executorDir = new File(workDir, appId + "/" + execId)
			val appLocalDirs = appDirectories.getOrElse(appId, {
				val localRootDirs = Utils.getOrCreateLocalRootDirs(conf)
				val dirs = localRootDirs.flatMap { dir => val appDir = Utils.createDirectory(dir, namePrefix = "executor") }.toSeq
				dirs
			})
			appDirectories(appId) = appLocalDirs
			val manager = new ExecutorRunner(appId, execId, appDesc.copy(command = Worker.maybeUpdateSSLSettings(appDesc.command, conf)), cores_, memory_, self, workerId, webUi.scheme, host, webUi.boundPort, publicAddress, sparkHome, executorDir, workerUri, conf, appLocalDirs, ExecutorState.LAUNCHING, resources_)
			executors(appId + "/" + execId) = manager
			manager.start()
	}
}
private[deploy] class org.apache.spark.deploy.worker.ExecutorRunner(val appId: String, val execId: Int, val appDesc: ApplicationDescription, val cores: Int, val memory: Int, val worker: RpcEndpointRef, val workerId: String, val webUiScheme: String, val host: String, val webUiPort: Int, val publicAddress: String, val sparkHome: File, val executorDir: File, val workerUrl: String, conf: SparkConf, val appLocalDirs: Seq[String], @volatile var state: ExecutorState.Value, val resources: Map[String, ResourceInformation] = Map.empty) extends Logging {
	private[worker] def start(): Unit = {
		workerThread = new Thread("ExecutorRunner for " + fullId) {
			override def run(): Unit = { fetchAndRunExecutor() }
		}
		workerThread.start()
	}
	private def fetchAndRunExecutor(): Unit = {
		val subsCommand = appDesc.command.copy(arguments = arguments, javaOpts = subsOpts)
		val builder = CommandUtils.buildProcessBuilder(subsCommand, new SecurityManager(conf), memory, sparkHome.getAbsolutePath, substituteVariables)
		val command = builder.command()
		val redactedCommand = Utils.redactCommandLineArgs(conf, command.asScala.toSeq).mkString("\"", "\" \"", "\"")
		process = builder.start()
		worker.send(ExecutorStateChanged(appId, execId, state, Some(message), Some(exitCode)))
	}
	
}





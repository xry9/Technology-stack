

private[deploy] class org.apache.spark.deploy.master.Master(override val rpcEnv: RpcEnv, address: RpcAddress, webUiPort: Int, val securityMgr: SecurityManager, val conf: SparkConf) extends ThreadSafeRpcEndpoint with Logging with LeaderElectable {
	override def receive: PartialFunction[Any, Unit] = {
		case RegisterApplication(description, driver) =>
		val app = createApplication(description, driver)
		registerApplication(app)
		driver.send(RegisteredApplication(app.id, self))
		schedule()
	}
	private def schedule(): Unit = {
		startExecutorsOnWorkers()
	}
	private def startExecutorsOnWorkers(): Unit = {
		for (app <- waitingApps if app.coresLeft > 0) {
			for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {
				allocateWorkerResourceToExecutors(app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))
			}
		}
	}
	private def allocateWorkerResourceToExecutors(app: ApplicationInfo, assignedCores: Int, coresPerExecutor: Option[Int], worker: WorkerInfo): Unit = {
		val numExecutors = coresPerExecutor.map { assignedCores / _ }.getOrElse(1)
		val coresToAssign = coresPerExecutor.getOrElse(assignedCores)
		for (i <- 1 to numExecutors) {
			val exec = app.addExecutor(worker, coresToAssign)
			launchExecutor(worker, exec)
		}
	}
	private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = {
		worker.addExecutor(exec)
		worker.endpoint.send(LaunchExecutor(masterUrl, exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))
		exec.application.driver.send(ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))
	}  
}


private[spark] class org.apache.spark.executor.CoarseGrainedExecutorBackend(override val rpcEnv: RpcEnv, driverUrl: String, executorId: String, hostname: String, cores: Int, userClassPath: Seq[URL], env: SparkEnv) extends ThreadSafeRpcEndpoint with ExecutorBackend with Logging {
	override def receive: PartialFunction[Any, Unit] = {
		case LaunchTask(data) =>
			val taskDesc = TaskDescription.decode(data.value)
			executor.launchTask(this, taskDesc)
	}
}


private[spark] class org.apache.spark.executor.Executor(executorId: String, executorHostname: String, env: SparkEnv, userClassPath: Seq[URL] = Nil, isLocal: Boolean = false, uncaughtExceptionHandler: UncaughtExceptionHandler = SparkUncaughtExceptionHandler) extends Logging {
	def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
		val tr = new TaskRunner(context, taskDescription)
		runningTasks.put(taskDescription.taskId, tr)
		threadPool.execute(tr)
	}
	class TaskRunner(execBackend: ExecutorBackend, private val taskDescription: TaskDescription) extends Runnable {
		override def run(): Unit = {
			val ser = env.closureSerializer.newInstance()
			task = ser.deserialize[Task[Any]](taskDescription.serializedTask, Thread.currentThread.getContextClassLoader)
			val res = task.run(taskAttemptId = taskId, attemptNumber = taskDescription.attemptNumber, metricsSystem = env.metricsSystem)
		}
	}
}
private[spark] abstract class org.apache.spark.scheduler.Task[T](val stageId: Int, val stageAttemptId: Int, val partitionId: Int, @transient var localProperties: Properties = new Properties, serializedTaskMetrics: Array[Byte] = SparkEnv.get.closureSerializer.newInstance().serialize(TaskMetrics.registered).array(), val jobId: Option[Int] = None, val appId: Option[String] = None, val appAttemptId: Option[String] = None) extends Serializable {
	final def run(taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem): T = {
		SparkEnv.get.blockManager.registerTask(taskAttemptId)
		context = new TaskContextImpl(stageId, partitionId, taskAttemptId, attemptNumber, taskMemoryManager, localProperties, metricsSystem, metrics)
		TaskContext.setTaskContext(context)
		new CallerContext("TASK", SparkEnv.get.conf.get(APP_CALLER_CONTEXT), appId, appAttemptId, jobId, Option(stageId), Option(stageAttemptId), Option(taskAttemptId), Option(attemptNumber)).setCurrentContext()
		runTask(context)
	}
}


private[spark] class org.apache.spark.scheduler.ShuffleMapTask(stageId: Int, stageAttemptId: Int, taskBinary: Broadcast[Array[Byte]], partition: Partition, @transient private var locs: Seq[TaskLocation], localProperties: Properties, serializedTaskMetrics: Array[Byte], jobId: Option[Int] = None, appId: Option[String] = None, appAttemptId: Option[String] = None) extends Task[MapStatus](stageId, stageAttemptId, partition.index, localProperties, serializedTaskMetrics, jobId, appId, appAttemptId) with Logging {
	override def runTask(context: TaskContext): MapStatus = {
		val ser = SparkEnv.get.closureSerializer.newInstance()
		val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
		var writer: ShuffleWriter[Any, Any] = null
		val manager = SparkEnv.get.shuffleManager
		writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
		writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
	}
}

private[spark] class org.apache.spark.scheduler.ResultTask[T, U](stageId: Int, stageAttemptId: Int, taskBinary: Broadcast[Array[Byte]], partition: Partition, locs: Seq[TaskLocation], val outputId: Int, localProperties: Properties, serializedTaskMetrics: Array[Byte], jobId: Option[Int] = None, appId: Option[String] = None, appAttemptId: Option[String] = None) extends Task[U](stageId, stageAttemptId, partition.index, localProperties, serializedTaskMetrics, jobId, appId, appAttemptId) with Serializable with Logging {
	override def runTask(context: TaskContext): U = {
		val ser = SparkEnv.get.closureSerializer.newInstance()
		val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
		func(context, rdd.iterator(partition, context))
	}
}

1„ÄÅrpc: org.apache.spark.rpc.netty.NettyRpcEnv#postToOutbox


public final class JavaWordCount {
  public static void main(String[] args) throws Exception {
    SparkSession spark = SparkSession.builder().appName("JavaWordCount").getOrCreate();
    JavaRDD<String> lines = spark.read().textFile(args[0]).javaRDD();
    JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>(){
      public Iterator<String> call(String s) throws Exception { return Arrays.asList(SPACE.split(s)).iterator(); }
    });
    JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(s, 1));
    JavaPairRDD<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>(){
      public Integer call(Integer v1, Integer v2) throws Exception { return v1 + v2; }
    });
    List<Tuple2<String, Integer>> output = counts.collect();
    for (Tuple2<?,?> tuple : output) { System.out.println(tuple._1() + ": " + tuple._2()); }
    spark.stop();
  }
}

class SparkContext(config: SparkConf) extends Logging {
    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
	_taskScheduler = ts
	_taskScheduler.start()
}

private[spark] class TaskSchedulerImpl(val sc: SparkContext, val maxTaskFailures: Int, isLocal: Boolean = false, clock: Clock = new SystemClock) extends TaskScheduler with Logging {
  override def start(): Unit = {
    backend.start()
  }
}


private[spark] class org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(scheduler: TaskSchedulerImpl, sc: SparkContext, masters: Array[String]) extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) with StandaloneAppClientListener with Logging {
  override def start(): Unit = {
    super.start()
    val command = Command("org.apache.spark.executor.CoarseGrainedExecutorBackend", args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)
    val appDesc = ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command, webUrl, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit, resourceReqsPerExecutor = executorResourceReqs)
    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)
    client.start()
  }
}

private[spark] class org.apache.spark.executor.CoarseGrainedExecutorBackend(override val rpcEnv: RpcEnv, driverUrl: String, executorId: String, bindAddress: String, hostname: String, cores: Int, userClassPath: Seq[URL], env: SparkEnv, resourcesFileOpt: Option[String], resourceProfile: ResourceProfile) extends IsolatedRpcEndpoint with ExecutorBackend with Logging {
	override def receive: PartialFunction[Any, Unit] = {
		case RegisteredExecutor =>
			executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false, resources = _resources)
			driver.get.send(LaunchedExecutor(executorId))	  
		case LaunchTask(data) =>
			val taskDesc = TaskDescription.decode(data.value)
			taskResources(taskDesc.taskId) = taskDesc.resources
			executor.launchTask(this, taskDesc)
	}
	override def onStart(): Unit = {
		rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>
			driver = Some(ref)
			ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls, extractAttributes, _resources, resourceProfile.id))
		}(ThreadUtils.sameThread).onComplete {
			case Success(_) =>{
				self.send(RegisteredExecutor)
			}
		}(ThreadUtils.sameThread)
	}
	
}

class org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: RpcEnv) extends ExecutorAllocationClient with SchedulerBackend with Logging {
	private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = {
		for (task <- tasks.flatten) {
			val serializedTask = TaskDescription.encode(task)
			val executorData = executorDataMap(task.executorId)
			val rpId = executorData.resourceProfileId
			val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)
			val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)
			executorData.freeCores -= taskCpus
			task.resources.foreach { case (rName, rInfo) =>
				executorData.resourcesInfo(rName).acquire(rInfo.addresses)
			}
			executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
		}
    }
	private def makeOffers(): Unit = {
		val taskDescs = withLock {
			val activeExecutors = executorDataMap.filterKeys(isExecutorActive)
			val workOffers = activeExecutors.map {
			  case (id, executorData) =>
				new WorkerOffer(id, executorData.executorHost, executorData.freeCores,
				  Some(executorData.executorAddress.hostPort),
				  executorData.resourcesInfo.map { case (rName, rInfo) =>
					(rName, rInfo.availableAddrs.toBuffer)
				  }, executorData.resourceProfileId)
			}.toIndexedSeq
			scheduler.resourceOffers(workOffers, true)
		}
		if (taskDescs.nonEmpty) {
			launchTasks(taskDescs)
		}
    }
	override def receive: PartialFunction[Any, Unit] = {
      case ReviveOffers =>
        makeOffers()
    }
	override def reviveOffers(): Unit = Utils.tryLogNonFatalError {
		driverEndpoint.send(ReviveOffers)
	}

}

private[spark] class org.apache.spark.executor.Executor(executorId: String, executorHostname: String, env: SparkEnv, userClassPath: Seq[URL] = Nil, isLocal: Boolean = false, uncaughtExceptionHandler: UncaughtExceptionHandler = new SparkUncaughtExceptionHandler, resources: immutable.Map[String, ResourceInformation]) extends Logging {
	def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
		val tr = new TaskRunner(context, taskDescription, plugins)
		runningTasks.put(taskDescription.taskId, tr)
		threadPool.execute(tr)
	}
  
	class TaskRunner(execBackend: ExecutorBackend, private val taskDescription: TaskDescription, private val plugins: Option[PluginContainer]) extends Runnable {
		override def run(): Unit = {
			val res = task.run(taskAttemptId = taskId, attemptNumber = taskDescription.attemptNumber, metricsSystem = env.metricsSystem, resources = taskDescription.resources, plugins = plugins)
		}
	}
}

private[spark] class org.apache.spark.scheduler.DAGScheduler(private[scheduler] val sc: SparkContext, private[scheduler] val taskScheduler: TaskScheduler, listenerBus: LiveListenerBus, mapOutputTracker: MapOutputTrackerMaster, blockManagerMaster: BlockManagerMaster, env: SparkEnv, clock: Clock = new SystemClock()) extends Logging {
	private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
		val tasks: Seq[Task[_]] = try {
		  val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
		  stage match {
			case stage: ShuffleMapStage =>
			  stage.pendingPartitions.clear()
			  partitionsToCompute.map { id =>
				val locs = taskIdToLocations(id)
				val part = partitions(id)
				stage.pendingPartitions += id
				new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
				  taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
				  Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
			  }
			case stage: ResultStage =>
			  partitionsToCompute.map { id =>
				val p: Int = stage.partitions(id)
				val part = partitions(p)
				val locs = taskIdToLocations(id)
				new ResultTask(stage.id, stage.latestInfo.attemptNumber,
				  taskBinary, part, locs, id, properties, serializedTaskMetrics,
				  Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
				  stage.rdd.isBarrier())
			  }
		  }
		}
		taskScheduler.submitTasks(new TaskSet(tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties, stage.resourceProfileId))
	}
	
	private def submitStage(stage: Stage): Unit = {
		val jobId = activeJobForStage(stage)
		if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
			val missing = getMissingParentStages(stage).sortBy(_.id)
			if (missing.isEmpty) {
			  submitMissingTasks(stage, jobId.get)
			} else {
			  for (parent <- missing) {
				submitStage(parent)
			  }
			  waitingStages += stage
			}
		}
	}
}

private[spark] class org.apache.spark.deploy.client.StandaloneAppClient(rpcEnv: RpcEnv, masterUrls: Array[String], appDescription: ApplicationDescription, listener: StandaloneAppClientListener, conf: SparkConf)  extends Logging {
	override def onStart(): Unit = {
		registerWithMaster(1)
    }
	private def registerWithMaster(nthRetry: Int): Unit = {
		registerMasterFutures.set(tryRegisterAllMasters())
    }
    private def tryRegisterAllMasters(): Array[JFuture[_]] = {
		for (masterAddress <- masterRpcAddresses) yield {
			registerMasterThreadPool.submit(new Runnable {
				override def run(): Unit = {
					val masterRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
					masterRef.send(RegisterApplication(appDescription, self))
				}
			})
		}
    }
}

private[netty] class org.apache.spark.rpc.netty.Inbox(val endpointName: String, val endpoint: RpcEndpoint) extends Logging {
	messages.add(OnStart)
	def process(dispatcher: Dispatcher): Unit = {
		var message: InboxMessage = null
		while (true) {
			safelyCall(endpoint) {
				message match {
					case RpcMessage(_sender, content, context) =>
						endpoint.receiveAndReply(context).applyOrElse[Any, Unit](content, { msg =>
						throw new SparkException(s"Unsupported message $message from ${_sender}")
						})
					case OnStart =>
						endpoint.onStart()
						if (!endpoint.isInstanceOf[ThreadSafeRpcEndpoint]) {
							if (!stopped) {
							  enableConcurrent = true
							}
						}
				}
			}
			message = messages.poll()
		}
	}
}


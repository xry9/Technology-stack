
private[spark] class org.apache.spark.executor.CoarseGrainedExecutorBackend(override val rpcEnv: RpcEnv, driverUrl: String, executorId: String, bindAddress: String, hostname: String, cores: Int, userClassPath: Seq[URL], env: SparkEnv, resourcesFileOpt: Option[String], resourceProfile: ResourceProfile) extends IsolatedRpcEndpoint with ExecutorBackend with Logging {
	override def receive: PartialFunction[Any, Unit] = {
		case RegisteredExecutor =>
			executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false, resources = _resources)
			driver.get.send(LaunchedExecutor(executorId))	  
		case LaunchTask(data) =>
			val taskDesc = TaskDescription.decode(data.value)
			taskResources(taskDesc.taskId) = taskDesc.resources
			executor.launchTask(this, taskDesc)		
	}
	override def onStart(): Unit = {
		rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap { ref =>
			driver = Some(ref)
			ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls, extractAttributes, _resources, resourceProfile.id))
		}(ThreadUtils.sameThread).onComplete {
			case Success(_) =>{
				self.send(RegisteredExecutor)
			}
		}(ThreadUtils.sameThread)
	}
	override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit = {
		val resources = taskResources.getOrElse(taskId, Map.empty[String, ResourceInformation])
		val msg = StatusUpdate(executorId, taskId, state, data, resources)
		driver match {
			case Some(driverRef) =>
				driverRef.send(msg)
		}
	}
}

private[spark] class org.apache.spark.executor.Executor(executorId: String, executorHostname: String, env: SparkEnv, userClassPath: Seq[URL] = Nil, isLocal: Boolean = false, uncaughtExceptionHandler: UncaughtExceptionHandler = new SparkUncaughtExceptionHandler, resources: immutable.Map[String, ResourceInformation]) extends Logging {
	def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
		val tr = new TaskRunner(context, taskDescription, plugins)
		runningTasks.put(taskDescription.taskId, tr)
		threadPool.execute(tr)
	}
	class TaskRunner(execBackend: ExecutorBackend, private val taskDescription: TaskDescription, private val plugins: Option[PluginContainer]) extends Runnable {
		override def run(): Unit = {
			execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)
			val res = task.run(taskAttemptId = taskId, attemptNumber = taskDescription.attemptNumber, metricsSystem = env.metricsSystem, resources = taskDescription.resources, plugins = plugins)
			execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)
		}
	}
}

private[spark] abstract class org.apache.spark.scheduler.Task[T](val stageId: Int, val stageAttemptId: Int, val partitionId: Int, @transient var localProperties: Properties = new Properties, serializedTaskMetrics: Array[Byte] = SparkEnv.get.closureSerializer.newInstance().serialize(TaskMetrics.registered).array(), val jobId: Option[Int] = None, val appId: Option[String] = None, val appAttemptId: Option[String] = None, val isBarrier: Boolean = false) extends Serializable {
	final def run(taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem, resources: Map[String, ResourceInformation], plugins: Option[PluginContainer]): T = {
		SparkEnv.get.blockManager.registerTask(taskAttemptId)
		val taskContext = new TaskContextImpl(stageId, stageAttemptId, partitionId, taskAttemptId, attemptNumber, taskMemoryManager, localProperties, metricsSystem, metrics, resources)
		context = if (isBarrier) { new BarrierTaskContext(taskContext) } else { taskContext }
		runTask(context)
	}
}

private[spark] class org.apache.spark.scheduler.ShuffleMapTask(stageId: Int, stageAttemptId: Int, taskBinary: Broadcast[Array[Byte]], partition: Partition, @transient private var locs: Seq[TaskLocation], localProperties: Properties, serializedTaskMetrics: Array[Byte], jobId: Option[Int] = None, appId: Option[String] = None, appAttemptId: Option[String] = None, isBarrier: Boolean = false) extends Task[MapStatus](stageId, stageAttemptId, partition.index, localProperties, serializedTaskMetrics, jobId, appId, appAttemptId, isBarrier) with Logging {
	override def runTask(context: TaskContext): MapStatus = {
		val ser = SparkEnv.get.closureSerializer.newInstance()
		val rddAndDep = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
		val rdd = rddAndDep._1
		val dep = rddAndDep._2
		dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition)
	}
}

private[netty] class org.apache.spark.rpc.netty.Inbox(val endpointName: String, val endpoint: RpcEndpoint) extends Logging {
	messages.add(OnStart)
	def process(dispatcher: Dispatcher): Unit = {
		var message: InboxMessage = null
		while (true) {
			safelyCall(endpoint) {
				message match {
					case RpcMessage(_sender, content, context) =>
						endpoint.receiveAndReply(context).applyOrElse[Any, Unit](content, { msg =>
						throw new SparkException(s"Unsupported message $message from ${_sender}")
						})
					case OnStart =>
						endpoint.onStart()
						if (!endpoint.isInstanceOf[ThreadSafeRpcEndpoint]) {
							if (!stopped) {
							  enableConcurrent = true
							}
						}
				}
			}
			message = messages.poll()
		}
	}
}
